import spacy
import re
import numpy as np
import gradio as gr
from transformers import pipeline
from rank_bm25 import BM25Okapi
from sentence_transformers import SentenceTransformer
import faiss
from sklearn.feature_extraction.text import TfidfVectorizer

# –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –º–æ–¥–µ–ª–µ–π
nlp = spacy.load('uk_core_news_sm')
bert_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
generator = pipeline('text2text-generation', model='facebook/bart-large-cnn')

# –ü–æ–ø–µ—Ä–µ–¥–Ω—è –æ–±—Ä–æ–±–∫–∞ —Ç–µ–∫—Å—Ç—É
def preprocess_text_spacy(text):
    text = text.lower()
    text = re.sub(r'\d+', '', text)
    doc = nlp(text)
    return ' '.join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.is_alpha])

# –ö–ª–∞—Å –¥–ª—è –ø–æ—à—É–∫—É
class TextRetriever:
    def __init__(self, corpus):
        self.original_corpus = corpus
        self.processed_corpus = [preprocess_text_spacy(doc) for doc in corpus]
        self.bm25 = BM25Okapi([doc.split() for doc in self.processed_corpus])
        self.faiss_index = self._build_faiss_index()

    def _build_faiss_index(self):
        embeddings = np.array(bert_model.encode(self.processed_corpus))
        index = faiss.IndexFlatL2(embeddings.shape[1])
        index.add(embeddings.astype('float32'))
        return index

    def search(self, query, top_n=3):
        processed_query = preprocess_text_spacy(query)

        # –ü–æ—à—É–∫ BM25
        bm25_docs = self.bm25.get_top_n(processed_query.split(), self.processed_corpus, n=top_n)

        # –ü–æ—à—É–∫ FAISS
        query_vector = bert_model.encode(query)
        _, indices = self.faiss_index.search(np.array([query_vector]).astype('float32'), top_n)
        faiss_docs = [self.processed_corpus[i] for i in indices[0] if i < len(self.processed_corpus)]

        # –û–±'—î–¥–Ω–∞–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
        combined = list({doc: None for doc in bm25_docs + faiss_docs}.keys())[:top_n]
        return [self.original_corpus[self.processed_corpus.index(doc)] for doc in combined]

# –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π
class ResponseGenerator:
    def generate_response(self, query, retrieved_docs):
        context = ' '.join(retrieved_docs)
        prompt = f'–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context} \n–ü–∏—Ç–∞–Ω–Ω—è: {query} \n–í—ñ–¥–ø–æ–≤—ñ–¥—å:'
        result = generator(prompt, max_length=150, num_beams=4, early_stopping=True)
        return result[0]['generated_text'].replace('–í—ñ–¥–ø–æ–≤—ñ–¥—å:', '').strip()

# –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —Å–∏—Å—Ç–µ–º–∏
corpus = [
    "–ú–∞—à–∏–Ω–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –∞–ª–≥–æ—Ä–∏—Ç–º–∏ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É —Ç–∞ –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è –¥–∞–Ω–∏—Ö.",
    "–®—Ç—É—á–Ω–∏–π —ñ–Ω—Ç–µ–ª–µ–∫—Ç –º–∞—î —à–∏—Ä–æ–∫–∏–π —Å–ø–µ–∫—Ç—Ä –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω—å —É –Ω–∞—É—Ü—ñ —Ç–∞ —Ç–µ—Ö–Ω—ñ—Ü—ñ.",
    "–¢–µ—Ö–Ω–æ–ª–æ–≥—ñ—ó –æ–±—Ä–æ–±–∫–∏ –ø—Ä–∏—Ä–æ–¥–Ω–æ—ó –º–æ–≤–∏ —Å–ø—Ä–∏—è—é—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü—ñ—ó –±–∞–≥–∞—Ç—å–æ—Ö –∑–∞–≤–¥–∞–Ω—å —É —Ä—ñ–∑–Ω–∏—Ö —Å—Ñ–µ—Ä–∞—Ö.",
    "–£ —Å–≤—ñ—Ç—ñ –∑—Ä–æ—Å—Ç–∞—î –ø–æ—Ç—Ä–µ–±–∞ –≤ —Å–∏—Å—Ç–µ–º–∞—Ö –¥–ª—è –æ–±—Ä–æ–±–∫–∏ –≤–µ–ª–∏–∫–∏—Ö –æ–±—Å—è–≥—ñ–≤ –¥–∞–Ω–∏—Ö.",
    "–í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –Ω–µ–π—Ä–æ–Ω–Ω–∏—Ö –º–µ—Ä–µ–∂ –¥–æ–∑–≤–æ–ª—è—î –∑–Ω–∞—á–Ω–æ –ø–æ–∫—Ä–∞—â–∏—Ç–∏ —Ç–æ—á–Ω—ñ—Å—Ç—å –º–æ–¥–µ–ª–µ–π —É —Ä—ñ–∑–Ω–∏—Ö –≥–∞–ª—É–∑—è—Ö."
]

retriever = TextRetriever(corpus)
response_generator = ResponseGenerator()

def process_query(query):
    retrieved_docs = retriever.search(query, top_n=3)
    response = response_generator.generate_response(query, retrieved_docs)
    return response, "\n\n".join(f"üìÑ {doc}" for doc in retrieved_docs)

# –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —ñ–Ω—Ç–µ—Ä—Ñ–µ–π—Å—É Gradio
with gr.Blocks(theme=gr.themes.Soft(), title="–ü—Ä–∏–≤—ñ—Ç!") as demo:
    gr.Markdown("# ü¶â –Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ–π–Ω–∞ –¢–µ—Ö–Ω–æ–ª–æ–≥—ñ—è –û–±—Ä–æ–±–∫–∏ –ù–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–∏—Ö –¢–µ–∫—Å—Ç—ñ–≤")
    gr.Markdown("–°–∏—Å—Ç–µ–º–∞ –¥–ª—è –ø–æ—à—É–∫—É —Ç–∞ –∞–Ω–∞–ª—ñ–∑—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó")

    with gr.Row():
        query_input = gr.Textbox(
            label="–í–≤–µ–¥—ñ—Ç—å –≤–∞—à –∑–∞–ø–∏—Ç:",
            placeholder="–ù–∞–ø—Ä–∏–∫–ª–∞–¥: –©–æ —Ç–∞–∫–µ —Ü–∏–≤—ñ–ª—å–Ω–∏–π –∫–æ–¥–µ–∫—Å?",
            scale=5
        )
        submit_btn = gr.Button("–ü–æ—à—É–∫", variant="primary", scale=1)

    with gr.Row():
        response_output = gr.Textbox(label="–í—ñ–¥–ø–æ–≤—ñ–¥—å", interactive=False)
        docs_output = gr.Textbox(label="–ó–Ω–∞–π–¥–µ–Ω—ñ –¥–æ–∫—É–º–µ–Ω—Ç–∏", interactive=False)

    examples = gr.Examples(
        examples=[
            ["–©–æ —Ç–∞–∫–µ –º–∞—à–∏–Ω–Ω–µ –Ω–∞–≤—á–∞–Ω–Ω—è?"],
            ["–Ø–∫ –®–Ü –¥–æ–ø–æ–º–∞–≥–∞—î –≤ –Ω–∞—É—Ü—ñ?"],
            ["–©–æ —Ç–∞–∫–µ –æ–±—Ä–æ–±–∫–∞ –ø—Ä–∏—Ä–æ–¥–Ω–æ—ó –º–æ–≤–∏?"]
        ],
        inputs=[query_input]
    )

    submit_btn.click(
        fn=process_query,
        inputs=query_input,
        outputs=[response_output, docs_output]
    )

if __name__ == "__main__":
    demo.launch()
